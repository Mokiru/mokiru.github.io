---
title: 协同过滤
date: 2025-06-08 15:00:00 +0800
categories: [ML, 半监督学习]
tags: [study]     # TAG names should always be lowercase
author: momochi
# authors: [xx,xx]
description: 介绍协同过滤，以及代码实现
comments: true # 评论
pin: false # top 
math: true
---

## 1. 介绍

以下指定用户行为为投票行为。

协同过滤的任务是根据来自其他用户样本或群体(用户数据库)的用户投票数据库，预测特定用户(活跃用户)的利用率。以下将会介绍两种通用的协同过滤算法。基于内存的算法通过整个用户数据库来进行预测。而基于模型的协同过滤算法则利用用户数据库来估计或学习一个模型，然后利用该模型进行预测。

协同过滤系统通常是通过隐式投票还是显式投票来区分的。显式投票是指用户有意识地按自己对标题地偏好进行投票，通常采用离散地数字标度。例如，GroupLens系统使用1-5的评分标准让用户对Netnews文章进行评分，用户在阅读完每篇文章后都会明确评分。隐式投票是指通过解释用户行为或选择来推断投票或偏好。隐式投票可以基于浏览数据(例如在网络应用中)、购买记录(例如在在线或传统商店中)或其他类型的信息访问模式。

无论是哪一种类型的投票数据，协同过滤算法都必须解决数据缺失的问题。即我们通常没有所有标题的完整投票集。我们不能假定项目是随机缺失的。在大多数应用中，用户会对他们访问过的项目进行投票，而且更有可能对他们喜欢的项目进行访问(和投票)。

## 2. 基于内存的算法

用户数据库由一组投票 $$ v_{i,j}$$组成，这些投票与用户 $$ i $$ 在项目 $$ j $$上的投票相对应。如果 $$ I_i$$ 是用户 $$i$$ 的投票集，那么我们可以将用户 $$i$$的平均投票定义为:

$$
\overline{v_i}=\frac{1}{|I_i|}\sum_{j\in I_i}v_{i,j}
$$

在基于内存的协同过滤算法中，我们根据关于活跃用户(用下标 $$a$$ 表示)的一些部分信息，以及从用户数据库中计算出的一组权重，来预测该用户的投票(评分)。我们假设活跃用户对项目$$j$$的预测评分 $$P_{a,j}$$是其他用户的评分的加权和：

$$
p_{a,j}=\overline{v_a}+\kappa\sum^{n}_{i=1}w(a,i)(v_{i,j}-\overline{v_i})\tag{1}
$$

其中，$$n$$是协同过滤数据库中权重不为零的用户数量，权重 $$w(i,a)$$可以反映每个用户 $$i$$与活跃用户之间的距离、相关性或相似性。$$\kappa$$是一个正态化因子，使得权重的绝对值总和统一。

### 2.1 相关性

这种统计协同过滤的通用公式化表达(与口头或定性描述相对)最早出现在 GroupLens 项目 的相关文献中，在该项目中，皮尔逊相关系数(Pearson correlation coefficient) 被定义为计算用户间权重的基础 [Resnick 等，1994]。用户 $$a $$与用户 $$i$$ 之间的相关性为：

$$
w(a,i)=\frac{\sum_{j}(v_{a,j}-\overline{v_a})(v_{i,j}-\overline{v_i})}{\sqrt{\sum_{j}(v_{a,j}-\overline{v_a})^2\sum_{j}(v_{i,j}-\overline{v_i})^2}}\tag{2}
$$

其中对$$j$$的求和是对用户$$a$$和$$i$$都有投票记录的项目求和

### 2.2 向量相似性

在信息检索领域，两个文档之间的相似性通常通过将每个文档视为词频向量，并计算这两个词频向量之间的夹角余弦值来衡量。我们可以将这种形式化方法应用到协同过滤中，在协同过滤中，用户相当于文档，项目标题相当于词语，用户评分相当于词频，需要注意的是，在该算法中，已观察到的评分表示一种正向偏好，负向偏好没有对应的角色，而未被评分的项目默认为$$0$$。此时，相关的权重为：

$$
w(a,i)=\sum_{j}\frac{v_{a,j}}{\sqrt{\sum_{k\in I_a}}v_{a,k}^{2}}\frac{v_{i,j}}{\sqrt{\sum_{k\in I_i}}v_{i,k}^{2}}\tag{3}
$$

其中，分母中的平方项用于对评分进行归一化处理，以确保那些对更多项目进行了评分的用户，并不会因此而与其他用户更加相似。其他归一化方式也是可行的，包括使用评分的绝对值之和、或评分数量等方法。

## 3. 基于内存的算法扩展

### 3.1 默认评分

默认评分是对相关性描述的相关算法(2.1)的一种扩展，源于一个观察结论：当活跃用户或匹配用户的评分数据相对较少时，相关性算法的效果会不理想，因为该算法仅使用两个用户都共同评过分的项目集合$$I_a\bigcap I_i$$来计算相似度。如果我们为那些没有显式评分的项目设定一个默认评分值，那么我们就可以在所有已评分项目的并集$$I_a\bigcup I_i$$上进行匹配计算，并将这些默认值代入公式中，用于替代未观测到的项目。

此外，我们可以假设对于一些额外的、两个用户都未曾评分的项目(数量为$$k$$)，它们的默认评分值也为$$d$$。这种做法相当于假定存在若干未明确列出的项目，虽然这两个用户都没有对它们进行评分，但可以认为他们在这些项目上的偏好是一致的。在大多数情况下，$$d$$的取值反映了对这些未观测项目的中性或略偏负面的偏好。

在使用隐式评分的应用场景中，一个观测到的评分通常表示一种正向偏好(例如，访问网页的行为会被赋予评分值$$1$$)。在这种情况下，默认评分可以设为“未访问”所对应的值，即$$0$$。此时，默认评分的作用就是用缺失数据的真实值来扩展每个用户的评分数据。需要注意的是，我们只对与活跃用户至少在一项上有共同评分的用户计算权重。

### 3.2 逆用户频率

在信息检索中，向量相似性方法通常会使用逆文档频率对词频进行加权，其核心思想是：对那些在多个文档中频繁出现的词语降低其权重，因为这些词语在识别文档主题时的作用较弱；而那些较少出现的词语则更能反映文档的主题。

我们可以将这一思想类比地应用于协同过滤中的评分数据，并称之为逆用户频率。其基本理念是：那些所有用户都喜欢的项目，在衡量用户间相似性时并不如那些较少被评分的项目有用。

我们定义$$f_i=\log\frac{n}{n_j}$$，其中$$n_j$$表示对项目$$j$$进行过评分的用户数量，$$n$$是数据库中用户的总数。注意，如果所有人都对某个项目$$j$$进行了评分，那么$$f_j$$的值为$$0$$。

在使用向量相似度算法时，我们将2.2中的公式的原始评分替换为经过变换的评分值，这个变换后的评分就是原始评分乘以$$$f_j$$因子。

在计算相关性时，我们对2.1中的公式调整，将$$f_j$$视为一种频率因子，即对于具有更高$$f_j$$的项目，在相关性计算中赋予更高的权重。引入逆频率因子后的相关性权重公式如下：

$$
w(a,i)=\frac{\sum_{j}f_j\sum_{j}v_{a,j}v_{i,j}-(\sum_{j}f_{j}v_{a,j})(\sum_{j}f_{j}v_{i,j})}{\sqrt{UV}}
$$

其中$$U=\sum_{j}f_{j}(\sum_{j}f_{j}v_{a,j}^{2}-(\sum_{j}f_{j}v_{a,j})^{2})$$，$$V=\sum_{j}f_{j}(\sum_{j}f_{j}v_{i,j}^{2}-(\sum_{j}f_{j}v_{i,j})^{2})$$

### 3.3 案例放大

案例放大指的是一种应用于基本协同过滤预测公式中所使用权重的变换方法，我们对估计出的权重进行如下变换：

$$
w_{a,i}^\prime =
\begin{cases}
w_{a,i}^\rho & \text{if } w_{a,i} \geq 0 \\
-(-w_{a,i})^\rho & \text{if } w_{a,i} < 0
\end{cases}
$$

这种变换会强调接近$$1$$的权重值，同时削弱较小的权重值。常用的$$\rho=2.5$$

## 4 基于模型的算法

从概率的角度看，协同过滤任务可以被看作是计算在已知用户信息的情况下某项评分的期望值。对于活跃用户，我们希望预测那些尚未评分的项目。如果我们假设评分是整数值，并且取值范围为0~m，则有：

$$
p_{a,j} = E(v_{a,j}) = \sum_{i=0}^{m} i \cdot Pr(v_{a,j} = i \mid v_{a,k}, k \in I_a)\tag{4}
$$

其中，$$p_{a,j}$$是活跃用户$$a$$对项目$$j$$的预测评分，$$Pr(v_{a,j} = i \mid v_{a,k}, k \in I_a)$$为在已知活跃用户对其他项目的评分情况下，该项目$$j$$的评分为$$i$$的概率。以下介绍两种用于协同过滤的概率模型：聚类模型和贝叶斯网络。

### 4.1 聚类模型

一种合理的协同过滤概率模型是贝叶斯分类器，该模型假设在给定一个未观测的类别变量$$C$$的条件下，所有评分之间是条件独立的。这个类别变量$$C$$取若干离散值中的一个。其核心思想是存在一些用户群体，它们共享一组共同的兴趣和偏好。给定类别后，各个项目(以评分形式表示)之间的偏好是相互独立的。

将联合概率与可操作的条件分布和边缘分布联系起来的概率模型是标准的“朴素”贝叶斯模型：

$$
Pr(C = c, v_1, ..., v_n) = Pr(C = c) \prod_{i=1}^n Pr(v_i \mid C = c)
$$

等式左边是观测到某个特定类别$$c$$和一组完整评分值的概率。在这个框架下，很容易计算公式(4)中所需的概率表达式，该模型也被称为多项混合模型，

模型的参数，类别成员概率$$Pr(C=c)$$，以及给定类别下的评分条件概率$$Pr(v_i\mid C=c)$$都是从训练集的用户评分数据中估计得到的。由于我们在用户数据库中从未直接观测到类别变量，因此必须使用能够处理隐藏变量模型的学习方法。

我们使用EM算法来学习固定类别数量结构的模型参数，通过选择使数据的(近似)边际似然最大的模型结构来确定类别数量。

### 4.2 贝叶斯网络模型

另一种用于概率协同过滤的模型形式是一个贝叶斯网络，其中每个项目对应一个节点。每个节点的状态对该项目的可能评分值，我们还引入了一个状态，表示“未评分”，用于那些缺失数据没有自然解释的领域。

然后我们对训练数据应用学习贝叶斯网络的算法，其中训练数据中的缺失评分用“未评分”值表示。

学习算法会搜索各种模型结构，以找到每个项目最适合的依赖关系。最终生成的网络中，每个项目都会有一组最佳预测它的父项目。每个条件概率表都由一棵决策树编码，表示该节点的条件概率。

## 5. 基于物品的协同过滤算法

基于物品的协同过滤关注目标用户已经评分的物品集合，并计算这些物品与目标物品之间的相似性，然后选择$$k$$个最相似的物品$$\{i_1,i_2,...,i_k\}$$。同时，也计算它们对应的相似度值$$\{s_{i_{1}},s_{i_{2}},...,s_{i_{k}}\}$$。一旦找到最相似的物品，预测值就通过对目标用户对这些相似物品的评分进行加权平均来计算，我们将从两个方面详细描述这个过程：
- 相似度计算
- 预测评分生成

### 5.1 物品相似性计算

在基于物品的协同过滤算法中，一个关键步骤是计算物品之间的相似性，并从中选择最相似的物品。在两个物品$$i$$和$$j$$之间计算相似性的基本思想是：首先找出对这两个物品都进行过评分的用户，然后应用某种相似性计算技术来确定物品$$i$$和$$j$$之间的相似度$$s_{i,j}$$。下图展示了这一过程，矩阵的行代表用户，列代表物品。

![alt text](/assets/img/ItemCF/Figure2.png)

有多种方式可以计算物品之间的相似性，以下介绍三种方法：
1. 基于余弦的相似性
2. 基于相关系数的相似性
3. 调整后的余弦相似性

#### 5.1.1 基于余弦的相似性

在这种情况下，将两个物品视为在$$m$$维用户空间中的两个向量。它们之间的相似性通过计算这两个向量之间的夹角的余弦来衡量。从形式上来看，上图所示的$$m\times n$$评分矩阵中，物品$$i$$和物品$$j$$之间的相似性表示为：

$$
sim(i,j)=cos(\overrightarrow{i}, \overrightarrow{j})=\frac{\overrightarrow{i}\cdot \overrightarrow{j}}{\mid\mid\overrightarrow{i}\mid\mid_{2}\cdot\mid\mid\overrightarrow{j}\mid\mid_{2}}
$$

#### 5.1.2 基于相关系数的相似性

在此情形下，物品$$i$$与物品$$j$$之间的相似性由皮尔逊相关系数$$corr_{i,j}$$来衡量。为了使相关性计算更准确，首先我们必须提取出那些对$$i$$和$$j$$都进行了评分的用户，如上图所示。设所有对物品$$i$$和$$j$$进行过评分的用户集合为$$U$$，则相似性表示为：

$$
sim(i,j)=\frac{\sum_{u\in U}(R_{u,i}-\overline{R}_{i})(R_{u,j}-\overline{R}_{j})}{\sqrt{\sum_{u\in U}(R_{u,i}-\overline{R}_{i})^{2}}\cdot\sqrt{\sum_{u\in U}(R_{u,j}-\overline{R}_{j})^{2}}}
$$

其中，$$R_{u,i}$$表示用户$$u$$对物品$$i$$的评分，$$\overline{R_{i}}$$表示物品$$i$$的平均评分。

#### 5.1.3 调整后的余弦相似性

基于用户的协同过滤与基于物品的协同过滤在相似性计算上的一个根本区别在于：
- 在基于用户的CF中，相似性是按照行方向计算的；
- 在基于物品的CF中，相似性是按照列方向计算的，即每一对共评分数据对应的是不同的用户，如上图。

使用基本的余弦相似性在基于物品的情形下有一个重要的缺点：没有考虑到不同用户之间的评分尺度的差异。调整后的余弦相似性通过从每个共评分数据中减去相应用户的平均评分来弥补这一缺陷，从形式上看，物品$$i$$和$$j$$之间的相似性可表示为：

$$
sim(i,j)=\frac{\sum_{u\in U}(R_{u,i}-\overline{R}_{u})(R_{u,j}-\overline{R}_u)}{\sqrt{\sum_{u\in U}(R_{u,i}-\overline{R}_{u})^{2}\cdot\sqrt{\sum_{u\in U}(R_{u,j}-\overline{R}_{u})^{2}}}}
$$

其中，$$\overline{R}_{u}$$表示用户$$u$$的平均评分

### 5.2 预测评分计算

在协同过滤系统中，最重要的一步是生成以预测形式呈现的输出接口。一旦我们根据相似性选出一组最相似的物品后，下一步就是查看目标用户的评分情况，并使用某种技术来获得预测结果。这里我们考虑两种这样的技术：

#### 5.2.1 加权求和法

该方法通过计算用户对物品$$i$$相似的其他物品的评分之和来进行预测，每个评分都乘以物品$$i$$与物品$$j$$之间的相似度$$s_{i,j}$$。使用下图的概念，我们可以将用户$$u$$对物品$$i$$的预测评分$$P_{u,i}$$表示为：

![alt text](/assets/img/ItemCF/Figure3.png)

$$
P_{u,i}=\frac{\sum_{所有相似物品N}s_{i,N}\cdot R_{u,N}}{\sum_{所有相似物品N}\mid s_{i,N}\mid}
$$

这种方法试图捕捉活跃用户如何评价这些相似物品，加权和通过相似度项的总和进行归一化，以确保预测结果落在预定义的评分范围内。

#### 5.2.2 回归模型

该方法类似于加权求和法，但不是直接使用相似物品的原始评分，而是基于回归模型来近似评分。在实践中，使用余弦或相关系数计算出的相似性可能产生误导，例如两个评分向量可能在欧几里得距离上相距较远，但相似度却很高。在这种情况下，使用所谓的相似物品的原始评分可能导致预测效果不佳。基本思想是使用与加权求和相同的公式，但用线性回归模型估算的评分值$$R^{\prime}_{N}=\alpha R_{i}+\beta +\epsilon$$。

其中，$$\alpha$$和$$\beta$$是通过遍历评分向量得到的回归参数；$$\epsilon$$是回归模型的误差。

### 5.3 性能影响分析

大型电子商务网站运行的规模给协同过滤的直接实现带来了巨大压力。在基于邻居的CF系统中，邻居生成过程，即用户-用户相似性计算步骤称为性能瓶颈，进而使得整个流程不适合实时生成推荐。

为了确保高可扩展性的一种方法是使用基于模型的方法。基于模型的系统具有使推荐系统在大规模场景下正常运行的潜力，其主要思路是将邻居生成和预测生成步骤分离。







