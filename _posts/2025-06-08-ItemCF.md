---
title: 协同过滤
date: 2025-06-08 15:00:00 +0800
categories: [机器学习, 推荐系统]
tags: [study]     # TAG names should always be lowercase
author: momochi
# authors: [xx,xx]
description: 介绍协同过滤，以及代码实现
comments: true # 评论
pin: false # top 
math: true
---

## 1. 介绍

以下指定用户行为为投票行为。

协同过滤的任务是根据来自其他用户样本或群体(用户数据库)的用户投票数据库，预测特定用户(活跃用户)对仙姑的利用率。以下将会介绍两种通用的协同过滤算法。基于内存的算法通过整个用户数据库来进行预测。而基于模型的协同过滤算法则利用用户数据库来估计或学习一个模型，然后利用该模型进行预测。

协同过滤系统通常是通过隐式投票还是显式投票来区分的。显式投票是指用户有意识地按自己对标题地偏好进行投票，通常采用离散地数字标度。例如，GroupLens系统使用1-5的评分标准让用户对Netnews文章进行评分，用户在阅读完每篇文章后都会明确评分。隐式投票是指通过解释用户行为或选择来推断投票或偏好。隐式投票可以基于浏览数据(例如在网络应用中)、购买记录(例如在在线或传统商店中)或其他类型的信息访问模式。

无论是哪一种类型的投票数据，协同过滤算法都必须解决数据缺失的问题。即我们通常没有所有标题的完整投票集。我们不能假定项目是随机缺失的。在大多数应用中，用户会对他们访问过的项目进行投票，而且更有可能对他们喜欢的项目进行访问(和投票)。

## 2. 基于内存的算法

用户数据库由一组投票 $$ v_i,j$$组成，这些投票与用户 $$ i $$ 在项目 $$ j $$上的投票相对应。如果 $$ I_i$$ 是用户 $$i$$ 的投票集，那么我们可以将用户 $$i$$的平均投票定义为:

$$
\overline{v_i}=\frac{1}{|I_i|}\sum_{j\in I_i}v_{i,j}
$$

在基于内存的协同过滤算法中，我们根据关于活跃用户(用下标 $$a$$ 表示)的一些部分信息，以及从用户数据库中计算出的一组权重，来预测该用户的投票(评分)。我们假设活跃用户对项目$$j$$的预测评分 $$P_{a,j}$$是其他用户的评分的加权和：

$$
p_{a,j}=\overline{v_a}+\kappa\sum^{n}_{i=1}w(a,i)(v_{i,j}-\overline{v_i})\tag{1}
$$

其中，$$n$$是协同过滤数据库中权重不为零的用户数量，权重 $$w(i,a)$$可以反映每个用户 $$i$$与活跃用户之间的距离、相关性或相似性。$$\kappa$$是一个正态化因子，使得权重的绝对值总和统一。

### 2.1 相关性

这种统计协同过滤的通用公式化表达(与口头或定性描述相对)最早出现在 GroupLens 项目 的相关文献中，在该项目中，皮尔逊相关系数(Pearson correlation coefficient) 被定义为计算用户间权重的基础 [Resnick 等，1994]。用户 $$a $$与用户 $$i$$ 之间的相关性为：

$$
w(a,i)=\frac{\sum_{j}(v_{a,j}-\overline{v_a})(v_{i,j}-\overline{v_i})}{\sqrt{\sum_{j}(v_{a,j}-\overline{v_a})^2\sum_{j}(v_{i,j}-\overline{v_i})^2}}\tag{2}
$$

其中对$$j$$的求和是对用户$$a$$和$$i$$都有投票记录的项目求和

### 2.2 向量相似性

在信息检索领域，两个文档之间的相似性通常通过将每个文档视为词频向量，并计算这两个词频向量之间的夹角余弦值来衡量。我们可以将这种形式化方法应用到协同过滤中，在协同过滤中，用户相当于文档，项目标题相当于词语，用户评分相当于词频，需要注意的是，在该算法中，已观察到的评分表示一种正向偏好，负向偏好没有对应的角色，而未被评分的项目默认为$$0$$。此时，相关的权重为：

$$
w(a,i)=\sum_{j}\frac{v_{a,j}}{\sqrt{\sum_{k\in I_a}}v_{a,k}^{2}}\frac{v_{i,j}}{\sqrt{\sum_{k\in I_i}}v_{i,k}^{2}}\tag{3}
$$

其中，分母中的平方项用于对评分进行归一化处理，以确保那些对更多项目进行了评分的用户，并不会因此而与其他用户更加相似。其他归一化方式也是可行的，包括使用评分的绝对值之和、或评分数量等方法。

## 3. 基于内存的算法扩展

### 3.1 默认评分

默认评分是对相关性描述的相关算法(2.1)的一种扩展，源于一个观察结论：当活跃用户或匹配用户的评分数据相对较少时，相关性算法的效果会不理想，因为该算法仅使用两个用户都共同评过分的项目集合$$I_a\bigcap I_i$$来计算相似度。如果我们为那些没有显式评分的项目设定一个默认评分值，那么我们就可以在所有已评分项目的并集$$I_a\bigcup I_i$$上进行匹配计算，并将这些默认值代入公式中，用于替代未观测到的项目。

此外，我们可以假设对于一些额外的、两个用户都未曾评分的项目(数量为$$k$$)，它们的默认评分值也为$$d$$。这种做法相当于假定存在若干未明确列出的项目，虽然这两个用户都没有对它们进行评分，但可以认为他们在这些项目上的偏好是一致的。在大多数情况下，$$d$$的取值反映了对这些未观测项目的中性或略偏负面的偏好。

在使用隐式评分的应用场景中，一个观测到的评分通常表示一种正向偏好(例如，访问网页的行为会被赋予评分值$$1$$)。在这种情况下，默认评分可以设为“未访问”所对应的值，即$$0$$。此时，默认评分的作用就是用缺失数据的真实值来扩展每个用户的评分数据。需要注意的是，我们只对与活跃用户至少在一项上有共同评分的用户计算权重。

### 3.2 逆用户频率

在信息检索中，向量相似性方法通常会使用逆文档频率对词频进行加权，其核心思想是：对那些在多个文档中频繁出现的词语降低其权重，因为这些词语在识别文档主题时的作用较弱；而那些较少出现的词语则更能反映文档的主题。

我们可以将这一思想类比地应用于协同过滤中的评分数据，并称之为逆用户频率。其基本理念是：那些所有用户都喜欢的项目，在衡量用户间相似性时并不如那些较少被评分的项目有用。

我们定义$$f_i=\log\frac{n}{n_j}$$，其中$$n_j$$表示对项目$$j$$进行过评分的用户数量，$$n$$是数据库中用户的总数。注意，如果所有人都对某个项目$$j$$进行了评分，那么$$f_j$$的值为$$0$$。

在使用向量相似度算法时，我们将2.2中的公式的原始评分替换为经过变换的评分值，这个变换后的评分就是原始评分乘以$$$f_j$$因子。

在计算相关性时，我们对2.1中的公式调整，将$$f_j$$视为一种频率因子，即对于具有更高$$f_j$$的项目，在相关性计算中赋予更高的权重。引入逆频率因子后的相关性权重公式如下：

$$
w(a,i)=\frac{\sum_{j}f_j\sum_{j}v_{a,j}v_{i,j}-(\sum_{j}f_{j}v_{a,j})(\sum_{j}f_{j}v_{i,j})}{\sqrt{UV}}
$$

其中$$U=\sum_{j}f_{j}(\sum_{j}f_{j}v_{a,j}^{2}-(\sum_{j}f_{j}v_{a,j})^{2})$$，$$V=\sum_{j}f_{j}(\sum_{j}f_{j}v_{i,j}^{2}-(\sum_{j}f_{j}v_{i,j})^{2})$$

### 3.3 案例放大

案例放大指的是一种应用于基本协同过滤预测公式中所使用权重的变换方法，我们对估计出的权重进行如下变换：

$$
w_{a,i}^\prime =
\begin{cases}
w_{a,i}^\rho & \text{if } w_{a,i} \geq 0 \\
-(-w_{a,i})^\rho & \text{if } w_{a,i} < 0
\end{cases}
$$

这种变换会强调接近$$1$$的权重值，同时削弱较小的权重值。常用的$$\rho=2.5$$

## 4 基于模型的算法

从概率的角度看，协同过滤任务可以被看作是计算在已知用户信息的情况下某项评分的期望值。对于活跃用户，我们希望预测那些尚未评分的项目。如果我们假设评分是整数值，并且取值范围为0~m，则有：

$$
p_{a,j} = E(v_{a,j}) = \sum_{i=0}^{m} i \cdot Pr(v_{a,j} = i \mid v_{a,k}, k \in I_a)\tag{4}
$$

其中，$$p_{a,j}$$是活跃用户$$a$$对项目$$j$$的预测评分，$$Pr(v_{a,j} = i \mid v_{a,k}, k \in I_a)$$为在已知活跃用户对其他项目的评分情况下，该项目$$j$$的评分为$$i$$的概率。以下介绍两种用于协同过滤的概率模型：聚类模型和贝叶斯网络。

### 4.1 聚类模型

一种合理的协同过滤概率模型是贝叶斯分类器，该模型假设在给定一个未观测的类别变量$$C$$的条件下，所有评分之间是条件独立的。这个类别变量$$C$$取若干离散值中的一个。其核心思想是存在一些用户群体，它们共享一组共同的兴趣和偏好。给定类别后，各个项目(以评分形式表示)之间的偏好是相互独立的。

将联合概率与可操作的条件分布和边缘分布联系起来的概率模型是标准的“朴素”贝叶斯模型：

$$
Pr(C = c, v_1, ..., v_n) = Pr(C = c) \prod_{i=1}^n Pr(v_i \mid C = c)
$$

等式左边是观测到某个特定类别$$c$$和一组完整评分值的概率。在这个框架下，很容易计算公式(4)中所需的概率表达式，该模型也被称为多项混合模型，

模型的参数，类别成员概率$$Pr(C=c)$$，以及给定类别下的评分条件概率$$Pr(v_i\mid C=c)$$都是从训练集的用户评分数据中估计得到的。由于我们在用户数据库中从未直接观测到类别变量，因此必须使用能够处理隐藏变量模型的学习方法。

我们使用EM算法来学习固定类别数量结构的模型参数，通过选择使数据的(近似)边际似然最大的模型结构来确定类别数量。

### 4.2 贝叶斯网络模型

另一种用于概率协同过滤的模型形式是一个贝叶斯网络，其中每个项目对应一个节点。每个节点的状态对该项目的可能评分值，我们还引入了一个状态，表示“未评分”，用于那些缺失数据没有自然解释的领域。

然后我们对训练数据应用学习贝叶斯网络的算法，其中训练数据中的缺失评分用“未评分”值表示。

学习算法会搜索各种模型结构，以找到每个项目最适合的依赖关系。最终生成的网络中，每个项目都会有一组最佳预测它的父项目。每个条件概率表都由一棵决策树编码，表示该节点的条件概率。











